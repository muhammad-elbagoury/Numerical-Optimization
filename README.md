# Numerical-Optimization
This repo contains implementations of different optimization techniques:
- Lab 1:
Batch Gradient Descent (GD).

- Lab 2:
Stochastic and Mini-Batch GD.

- Lab 3:
Momentum and NAD.

- Lab 4:
Adagrad, RMSProb, and Adam.
